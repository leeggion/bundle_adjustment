# Bundle Adjustment на C++

**Цель:** Реализовать с нуля алгоритм Bundle Adjustment (BA) и сравнить эффективность методов Гаусса-Ньютона (GN) и Левенберга-Марквардта (LM) на различных наборах данных.

## 1. Краткая теория

### 1.1. Что такое Bundle Adjustment (BA)?

**Bundle Adjustment** — это задача нелинейной оптимизации, которая является "золотым стандартом" и, как правило, последним шагом в алгоритмах 3D-реконструкции (Structure from Motion)[cite: 4, 32].

* **Цель:** Одновременно уточнить (оптимизировать) 3D-координаты точек сцены (структуру) и параметры камер (позы, внутреннюю калибровку)[cite: 35].
* **Критерий:** Минимизация **ошибки репроекции** (reprojection error)[cite: 42, 224].
* **Ошибка репроекции:** Это 2D-расстояние (в пикселях) между наблюдаемой проекцией точки на изображении ($x_{ij}$) и предсказанной проекцией ($\hat{x}_{ij}$), которая вычисляется путем проецирования 3D-точки ($b_i$) на камеру ($a_j$) с текущими параметрами[cite: 42, 224, 227].
* **Математически:** Мы минимизируем сумму квадратов этих ошибок[cite: 42, 225]:
    $
    \min_{a_j, b_i} \sum_{i=1}^{n} \sum_{j=1}^{m} v_{ij} \cdot d(Q(a_j, b_i), x_{ij})^2
    $
    Где:
    * $a_j$ — параметры камеры $j$ (поза + калибровка)[cite: 221].
    * $b_i$ — параметры 3D-точки $i$ (координаты X, Y, Z)[cite: 221].
    * $Q(a_j, b_i)$ — функция проекции, предсказывающая 2D-положение $x_{ij}$[cite: 227].
    * $v_{ij}$ — бинарный флаг (1, если точка $i$ видна в камере $j$, иначе 0)[cite: 222, 231].
    * $d(\cdot, \cdot)$ — Евклидово расстояние[cite: 227].

Это классическая задача **нелинейных наименьших квадратов (Non-linear Least Squares, NLS)**[cite: 4, 43, 115].

---

### 1.2. Решение NLS: Гаусс-Ньютон vs. Левенберг-Марквардт

Оба метода — это итеративные подходы для решения $\min ||\epsilon(p)||^2$, где $p$ — вектор всех параметров ($a_j$ и $b_i$), а $\epsilon$ — вектор всех ошибок репроекции[cite: 114, 124].

На каждой итерации мы ищем шаг $\delta_p$ для обновления параметров: $p_{new} = p + \delta_p$ [cite: 133-134].

#### 1.2.1. Метод Гаусса-Ньютона (GN)

GN линеаризует функцию ошибки $\epsilon(p+\delta_p) \approx \epsilon(p) + J\delta_p$, где $J$ — **Якобиан** (матрица первых производных $\frac{\partial \epsilon}{\partial p}$) [cite: 125, 127-129].

Для нахождения $\delta_p$ мы решаем **нормальные уравнения** [cite: 135-136]:

$$
(J^T J) \delta_p = J^T \epsilon
$$

* **$J^T J$** — это аппроксимация Гессиана (матрицы вторых производных)[cite: 138].
* **$J^T \epsilon$** — (минус) градиент[cite: 139].

**Проблема:** $J^T J$ может быть сингулярной (необратимой) или плохо обусловленной, особенно если мы далеко от минимума[cite: 150]. В этом случае метод расходится.

#### 1.2.2. Метод Левенберга-Марквардта (LM)

LM — это "гибрид" метода Гаусса-Ньютона и метода градиентного спуска[cite: 116]. Он более робастный и является стандартом для BA[cite: 43].

LM вводит **коэффициент демпфирования (damping)** $\mu$ в нормальные уравнения[cite: 143]. Мы решаем **дополненные нормальные уравнения** [cite: 140-141]:

$$
(J^T J + \mu I) \delta_p = J^T \epsilon
$$

* **Если $\mu$ велико:** $J^T J$ игнорируется, и $\delta_p \approx \frac{1}{\mu} (J^T \epsilon)$. Это шаг в сторону градиентного спуска — медленно, но надежно[cite: 148].
* **Если $\mu$ мало (близко к 0):** Уравнение превращается в чистый Гаусс-Ньютон, который быстро сходится вблизи минимума[cite: 152].

**Ключевая идея LM:** Алгоритм *адаптивно* подбирает $\mu$[cite: 153].
1.  Вычисляем $\delta_p$ с текущим $\mu$[cite: 171].
2.  Если ошибка $|| \epsilon(p+\delta_p) ||^2$ уменьшилась — принимаем шаг и *уменьшаем* $\mu$ (становимся "жаднее", ближе к GN)[cite: 144, 178, 183].
3.  Если ошибка увеличилась — отвергаем шаг и *увеличиваем* $\mu$ (становимся "осторожнее", ближе к градиентному спуску)[cite: 145, 184].

---

### 1.3. Ключевая особенность BA: Разреженность (Sparsity)

Это самая важная часть для *эффективной* реализации.

*  **Проблема:** Вектор $p$ огромен (тысячи камер + миллионы точек) [cite: 233-234]. Якобиан $J$ и $J^T J$ не поместятся в память, а решение $O(N^3)$ займет вечность[cite: 5, 45, 53].
* **Решение:** $J$ — **экстремально разреженная** (sparse) матрица[cite: 6].
* **Почему?** Ошибка репроекции $\hat{x}_{ij}$ зависит *только* от параметров камеры $j$ ($a_j$) и 3D-точки $i$ ($b_i$). Она *не* зависит ни от какой другой камеры $k \ne j$ или точки $k \ne i$[cite: 54, 249].
* **Результат:** Все производные в Якобиане, кроме $\frac{\partial \hat{x}_{ij}}{\partial a_j}$ и $\frac{\partial \hat{x}_{ij}}{\partial b_i}$, равны нулю [cite: 253-254].

Из-за этой структуры матрица $J^T J$ также имеет специальную разреженную блочную структуру [cite: 54, 305-323]. Вместо того чтобы решать гигантскую систему "в лоб", мы можем использовать **трюк с дополнением Шура (Schur complement)**[cite: 389].

Это позволяет разбить систему на две:
1.  Сначала решить *маленькую* систему (размером с кол-во камер $\times$ кол-во камер) относительно параметров камер $\delta_a$[cite: 387, 390].
2.  Затем, зная $\delta_a$, очень быстро (параллельно!) найти параметры точек $\delta_b$[cite: 391, 396].

**Ваш проект должен использовать разреженные матрицы.**

## 2. План действий по проекту (C++)

### Этап 1: "Каркас" и структуры данных

Создайте базовые классы/структуры для хранения данных.

1.  **Математика:** Вам *крайне* рекомендуется использовать библиотеку для линейной алгебры. **Eigen** — идеальный выбор (header-only, быстрая, отлично работает с разреженными матрицами). Реализовывать свои матричные операции с нуля — путь к страданиям.
2.  **Структуры:**
    * `CameraParameters`: Хранит внутренние параметры (фокусное $f$, центр $c_x, c_y$) и внешние (позу).
    * **Поза (Pose):** Для вращения используйте **кватернионы** (4 параметра) или векторы ось-угол (3 параметра). Это стабильнее, чем матрицы вращения. Плюс 3 параметра на трансляцию (t). Итого 6-7 параметров на камеру[cite: 709, 711].
    * `Point3D`: `double X, Y, Z` (3 параметра)[cite: 711].
    * `Observation`: `int camera_id`, `int point_id`, `double u, v` (2D-координаты).
3.  **Класс `BundleAdjustmentProblem`:**
    * Хранит `std::vector<CameraParameters> cameras`.
    * Хранит `std::vector<Point3D> points`.
    * Хранит `std::vector<Observation> observations`.
    * Содержит **вектор всех параметров $p$**: конкатенация всех параметров камер и точек[cite: 237].

### Этап 2: Функция проекции и Якобиан

Это ядро вашей оптимизации.

1.  **Функция проекции `project(cam, point)`:**
    * На вход: `CameraParameters`, `Point3D`.
    * Действия:
        1.  Преобразует 3D-точку из мировых координат в координаты камеры (используя кватернион и трансляцию)[cite: 713].
        2.  Применяет перспективное деление (деление на $Z_c$).
        3.  Применяет внутренние параметры (матрицу $K$) для получения пиксельных координат $(u, v)$[cite: 713].
    * На выход: 2D-точка $(u_{pred}, v_{pred})$.
2.  **Вычисление Якобиана (самая сложная часть):**
    * Вам нужны **аналитические** производные (не численные, они медленные и неточные)[cite: 593].
    * Вам нужно вычислить 2 блока производных для *каждого наблюдения*[cite: 254]:
        * `J_cam` ($\frac{\partial \hat{x}_{ij}}{\partial a_j}$): $2 \times 7$ матрица (производная 2D-точки по 7 параметрам камеры).
        * `J_point` ($\frac{\partial \hat{x}_{ij}}{\partial b_i}$): $2 \times 3$ матрица (производная 2D-точки по 3 параметрам 3D-точки).
    * Используйте **цепное правило (chain rule)**. Вам придется найти производные кватернионов, проекции и т.д. *Это потребует аккуратной математики.*[cite: 721].
3.  **Сборка Якобиана `J`:**
    * Создайте **разреженную матрицу** `Eigen::SparseMatrix<double> J`.
    * Ее размер: `(2 * num_observations) x (7 * num_cameras + 3 * num_points)`.
    * В цикле по всем `observations[k]`:
        1.  Вычисляете `J_cam` и `J_point`.
        2.  Вставляете эти маленькие блоки в большую разреженную матрицу $J$.

### Этап 3: Реализация солверов

Создайте базовый класс `Optimizer` и два наследника.

1.  **`GaussNewtonOptimizer`:**
    * В цикле (пока не сойдется):
        1.  Собрать $J$ и вектор ошибок $\epsilon$.
        2.  Вычислить $H = J^T J$ и $g = J^T \epsilon$.
        3.  Решить $H \delta_p = g$ (используя `Eigen::SparseLU` или `SimplicialLLT`).
        4.  Обновить параметры: $p = p + \delta_p$.
        5.  Проверить сходимость (например, $||\delta_p|| < \text{threshold}$).

2.  **`LevenbergMarquardtOptimizer`:**
    * В цикле (внешний цикл)[cite: 168]:
        1.  Собрать $J$ и $\epsilon$. Вычислить $H = J^T J$ и $g = J^T \epsilon$[cite: 165, 182].
        2.  Вычислить $\mu$ (например, $\mu = \tau \cdot \max(H.diagonal())$)[cite: 167].
        3.  **Внутренний цикл (поиск $\delta_p$):** [cite: 170]
            a. Создать разреженную единичную $I$.
            b. Решить $(H + \mu I) \delta_p = g$[cite: 171].
            c. Вычислить новую ошибку $|| \epsilon(p+\delta_p) ||^2$[cite: 176].
            d. Если ошибка *уменьшилась*: принять шаг $p = p + \delta_p$ [cite: 181], *уменьшить* $\mu$ (напр., $\mu /= 3$)[cite: 183], `break` (выход из внутреннего цикла).
            e. Если ошибка *увеличилась*: шаг *не* принимать, *увеличить* $\mu$ (напр., $\mu *= 2$)[cite: 184], `continue` (новая попытка во внутреннем цикле).
    * Проверить сходимость (например, $||g||_\infty < \text{threshold}$)[cite: 167].

### Этап 4: Данные и тестирование

1.  **Загрузчик данных:** Начните с данных в формате **BAL (Bundle Adjustment in the Large)**. Они идеально подходят: текстовые файлы, содержат все камеры, точки и наблюдения.
2.  **Синтетические данные:** Создайте "идеальный" мир (камеры, точки). Спроецируйте точки, чтобы получить "чистые" наблюдения. Затем добавьте Гауссовский шум к 2D-наблюдениям. Это позволит вам проверить, что ваш BA возвращает параметры, близкие к "идеальным".
3.  **Эксперимент:**
    * **Данные:**
        * Синтетика (с разным уровнем шума).
        * Реальные данные (напр., `ladybug` из BAL).
    * **Сравнение:** Для GN и LM на каждом наборе данных измерьте:
        1.  **Время сходимости.**
        2.  **Количество итераций.**
        3.  **Конечная ошибка (RMSE).**
        4.  **Робастность:** Добавьте шум к *начальным* параметрам камер и точек. Посмотрите, сойдется ли GN. (Спойлер: LM сойдется[cite: 43], а GN, скорее всего, нет).

### Этап 5: (Опционально) Оптимизация с дополнением Шура

Если базовая реализация с `Eigen::SparseLU` работает медленно на больших данных, реализуйте трюк с дополнением Шура (как описано в [cite: 389-397]). Это ускорит решение линейной системы на порядок[cite: 397].
